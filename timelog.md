# Timelog

* An investigation into feedback and representations in deep vision networks
* Martin Dimitrov
* 2247164D
* Jan Paul Siebert

## Guidance

* This file contains the time log for your project. It will be submitted along with your final dissertation.
* **YOU MUST KEEP THIS UP TO DATE AND UNDER VERSION CONTROL.**
* This timelog should be filled out honestly, regularly (daily) and accurately. It is for *your* benefit.
* Follow the structure provided, grouping time by weeks.  Quantise time to the half hour.

## Week 1 - 12h 19 mins

### 24 Sep 2019

* *4 hours* Attended taught part of project
* *1 hour* Read the project slides at home
* *5 min* Put names and titles into the template files
* *5 min* Set a repo
* *5 min* Set Trello board and Zotero (reference manager)
* *1 hour* Read abstract and conclusion of PhD Thesis

### 25 Sep 2019

* *1h 30 min* meeting with supervisor
* *20 min* Setup a template with references in Notion

### 27 Sep 2019
* *1h 15 min* read the Deep Predictive Coding Paper

### 28 Sep 2019
* *1h* Read Phd Thesis
* *30 min* Implemented my first neural network and trained it on MNIST following a tutorial

### 29 Sep 2019
* *24 min* Read an article on neural network theory
* *1h* Finished chapter 1 of PhD Thesis on cortical feedback

## Week 2 - 7h 26 min

### 30 Sep 2019
* *40 min* Looked at code for the Deep Predictive Coding paper and generally how to write code for neural networks

### 1 Oct 2019
* *1h* Read a MSc student's paper on Deep Predictive Coding based on retino-cortical transform
* *1h* Group Meeting with supervisor

### 2 Oct 2019
* *1h 15 min* Read Chapter 2 of the PhD Thesis
* *1h* Read an article which explains neural networks in depth

### 3 Oct 2019
* *20 min* Started reading chapter 3 of PhD Thesis

### 5 Oct 2019
* *45 min* Sorted out minutes from first two meetings and also sorted out some resources in Notion
* *25 min* Read PhD Thesis
* *9 min* Read an article on object detection models such as R-CNN and YOLO

### 5 Oct 2019
* *37 min* Finished Chapter 3 Of Thesis
* *30 min* Ran the code for the Deep Predictive Coding paper on Colab

### Week 3 - 15h 9 min

### 7 Oct 2019
* *1h* Read a paper on Deep Predictive Coding using Local Recurrent Processing
* *15 min* Autoencoders tutorial
* *11 min* Ran the code for the paper
* *53 min* Finished chapter 4 of thesis

### 8 Oct 2019
* *23 min* Wrote a high-level description of the project
* *1h* Meeting with supervisor

### 10 Oct 2019
* *31 min* Read a paper on cortical feedback signals
* *1h 24 min* Experimented running TensorBoard with PyTorch
* *37 min* Read a paper on attentive GAN

### 12 Oct 2019
* *23 min* Read a paper on R-CNN
* *1h 1 min* Read a paper on SegNet
* *15 min* Researched how to implement heatmaps using PyTorch, found out that this is called Grad-CAM
* *23 min* Did a tutorial on how to use heatmaps on images

### 13 Oct 2019
* *3h* Looked in depth on how to implement code for a simple neural network
* *4h* Autoencoder on the CIFAR10 Dataset

### Week 4 - 11h 10 min

### 14 Oct 2019
* *25 min* Looked at a paper which discussed class activation mapping
* *1h* Tried implementing CAM on the CIFAR dataset

### 15 Oct 2019
* *1h* Meeting with supervisor

### 16 Oct 2019
* *5h* Read a few chapters of a book on Neural Networks to get familiar with all the theory
* *3h* Experimented with an autoencoder on CIFAR

### 20 Oct 2019
* *38 min* Read articles and tutorials on CAM, collated CAM papers
* *14 min* Read an article on autoencoders

### Week 5 - 12h 5 min

### 21 Oct 2019
* *14 mins* Read an example on how to build an autoencoder
* *39 mins* Read a paper on Grad CAM

### 22 Oct 2019
* *1h* Read paper on GradCAM
* *20 min* Sorted out papers in Mendeley
* *30 min* Optmized code on autoencoder and classifier, looked on how to load and save models
* *45 min* Read paper on CAM

### 23 Oct 2019
* *2h 46 min* Worked on integrating GradCAM into my codebase

### 24 Oct 2019
* *1h* Worked on integrating GradCAM into my codebase
* *1h 30 min* Meeting with supervisor

### 26 Oct 2019
* *15 min* Tried to put together autoencoder and classifier in one model
* *2h 15 min* Did tutorials on CNN, Autoencoder and Autoencoder as Classifier

### 27 Oct 2019
* *3h 50 min* Put together autoencoder and classifier in one model

### Week 6 - 9h 25 min

### 29 Oct 2019
* *1h 23 min* Read a paper on Attention GAN

### 30 Oct 2019
* *1h 50 min* Optimized code in notebook

### 31 Oct 2019
* *34 min* Looked into the computation of losses
* *4 min* Organized notes
* *8 min* Looked at how to upload files onto Colab
* *11 min* Looked at increasing the batch size
* *1h* Meeting with supervisor

### 1 Nov 2019
* * 15 min* Did a tutorial on TorchSnooper

### 3 Nov 2019
* *1h* Introduced validation dataset
* *1h 9 min* Looked at baselines, improved code in general
* *1h 48 min* Converted networks to convolutional ones

### Week 7 - 14h 21 min

### 4 Nov 2019
* *50 min* Made a combined convolutional model
* *21 min* Plotted graphs
* *1h* Modified calculation of loss

### 5 Nov 2019
* *1h 11 min* Tried to fix overfitting
* *2h* Meeting with supervisor

### 7 Nov 2019
* *41 min* Read a paper on recurrent models and attention

### 9 Nov 2019
* *28 min* Rearranged code, checked if losses are consistent, tried incorporating Tensorboard into my code
* *2h 30 min* Compared different models
* *2h 12 min* Refactored code

### 10 Nov 2019
* *38 min* Organized papers, searched for papers on deep predictive coding
* *43 min* Read a paper on deep predictive coding
* *3h* Switched from Fashion MNIST to CIFAR

### Week 8 - 15h 27 min

### 11 Nov 2019
* *42 min* Read a paper on Human Attention in Visual Question Answering
* *45 min* Collated papers
* *42 min* Read a paper on Feedback connections
* *12 min* Implemented loss switch

### 12 Nov 2019
* *2h 45 min* Tried to improve CIFAR
* *1h* Meeting with supervisor
* *1h 45 min* Tried to overcome overfitting

### 13 Nov 2019
* *1h 7 min* Read Residual Attention Network

### 14 Nov 2019
* *48 min* Read a paper on Scale-aware Semantic Image Segmentation
* *38 min* Read a paper on feedback

### 15 Nov 2019
* *1h* Refactored code, included a plot on accuracy

### 16 Nov 2019
* *1h 27 min* Finished reading paper on feedback
* *1h* Read Capturing Top-Down Visual Attention
* *40 min* Read Attentional Neural Network
* *54 min* Read Learning with rethinking

### 17 Nov 2019
* *10 min* Imported dissertation template in Overleaf

### Week 9 - 6h 25 min

### 18 Nov 2019
* *2h* Tried to improve the accuracy on the network, looked at effects on leaky and non leaky
* *2h* Scaled losses

### 19 Nov 2019
* *2h* Meeting with supervisor

### 24 Nov 2019
* *25 min* Modified style of networks


### Week 10 - 8h 54 min

### 25 Nov 2019
* *6h* Tried implementing feedback on FashionMNIST, saw results on CIFAR

### 26 Nov 2019
* *1h* Tried implementing feedback

### 1 Dec 2019
* *1h 54 min* Cleaned up code to find where size error is coming from


### Week 11 - 3h

### 2 Dec 2019
* *2h 30 min* Combined my classifier with GRAD CAM on CIFAR

### 3 Dec 2019
* *30 min* Converted code to take images from the test set directly

### Week 12 - 5h 15 min

### 12 Dec 2019

* *1h* Started work on status report
* *1h 15 min* Read Feedback CNN for Visual Localization and Segmentation

### 13 Dec 2019
* *30 min* Experimented with subtracting images
* *1h* Meeting with supervisor

### 15 Dec 2019
* *30 min* Did a tutorial on intermediate activations
* *1h* Experimented with subtracting

### Week 13 - 2h

### 17 Dec 2019
* *1h* Evaluated results on different ranges
* *1h* Expanded status report

### Holiday Break - 10h 30 min

### 27 Dec 2019
* *42 min* Finished reading Visual Localization and Segmentation paper
* *1h 2 min* Started reading a hierarchical neural networks book

### 28 Dec 2019
* *23 min* Read the dissertation template and looked at latex syntax

### 29 Dec 2019
* *1h 3 min* Read Chapter 2 of the hierarchical NN book
* *1h 24 min* Looked at results on CIFAR

### 30 Dec 2019
* *2h* Improved models on CIFAR
* *17 min* Explored options for other datasets
* *30 min* Evaluated results on SVHN dataset

### 2 Jan 2020
* *1h 25 min* Read unsupervised image-to-image translation
* *15 min* Refactored plotting and saving code

### 3 Jan 2020
* *30 min* Tested results with a pretrained resnet on classifier

### 4 Jan 2020
* *2h* Modified pretrained resnet, used resnet for autoencoder and joint model

### 5 Jan 2020
* *20 min* Used resnet on subtracting generated output from input, changed activation to tanh

### 7 Jan 2020
* *40 min* Experimented with different values for weight decay

### Week 14 - 2h 30 min

### 13 Jan 2020
* *1h 30 min* Evaluated different architectures implemented so far, freezed fully connected layers

### 14 Jan 2020
* *1h* Meeting with supervisor

### 19 Jan 2020
* *1h* Tried multiplication instead of subtraction, initialized weights to zero
* *40 min* Read Alvaro's paper


### Week 15 - 7h 21 min

### 20 Jan 2020
* *4h* Pretrained classifier and freezed in joint network

### 21 Jan 2020
* *21 min* Ran Alvaro's scripts

### 25 Jan 2020
* *3h* Collated results from experiments

### Week 16 - 12h

### 27 Jan 2020
* *4h* Experimented with GradCAM and managed to bring out the foreground of the image?

### 30 Jan 2020
* *4h* Used foreground as an input to the network

### 31 Jan 2020
* *2h* Tried to superimpose gradcam and original image 

### 2 Feb 2020
* *2h* Evaluated models using GradCAM

### Week 17 - 21h

### 3 Feb 2020
* *4h* Evaluated the model "image - image*clip(1-gradcam)*alpha"
* *2h* Evaluated this model on CIFAR and SVHN

### 4 Feb 2020
* *2h* Implemented the model "image - clip(image*(1-gradcam)*alpha"
* *2h* Evaluated this model on CIFAR and SVHN

### 5 Feb 2020
* *5h* Implemented a model which takes the output of the generator and uses it as a second channel
* *3h* Evaluated this model on CIFAR and SVHN

### 7 Feb 2020
* *2h* Collated results in a slideshow
* *1h* Meeting with supervisor

### 8 Feb 2020
* *3h* Included a graph which showed the accuracy per feedback iteration on test set

### Week 18 - 16h

### 11 Feb 2020
* *4h* Collated results on subtractive and multiplicative feedback on CIFAR and SVHN for a different number of epochs and iterations

### 12 Feb 2020
* *2h* Collated results on using generator output as a second channel on CIFAR and SVHN for different epochs and iterations

### 14 Feb 2020
* *1h* Collated results in a slideshow
* *1h* Meeting with supervisor

### 16 Feb 2020
* *3h* Downloaded images from Alvaro's dataset, used a subset of them and uploaded onto Google Drive
* *5h* Evaluated models implemented so far on the dataset

### Week 19 - 16h

### 17 Feb 2020
* *7h* Used a pretrained classifier on feedback model with all layers upto last output layer frozen, then finetuned by unfreezing the layers

### 18 Feb 2020
* *1h* Meeting with supervisor

### 22 Feb 2020
* *3h* Evaluated results on CIFAR, SVHN and Alvaro's dataset

### 23 Feb 2020
* *4h* Extended classifier to have more convolutional layers and modified strides and padding
* *1h* Experimented with calculating strides and padding to ensure theoretical knowledge

### Week 20 - 8h 45 min

### 25 Feb 2020
* *3h* Evaluated results on the expanded classifier
* *1h* Meeting with supervisor

### 27 Feb 2020
* *2h* Collated dataset of pictures of fruits

### 1 March 2020
* *2h* Evaluated baselines on fruits set
* *45 min* Wrote an initial outline of the dissertation

### Week 21 - 4h

### 2 March 2020
* *1h* Finished outline of dissertation

### 3 March 2020
* *1h* Meeting with supervisor

### 7 March 2020
* *2h* Cleaned up code

### Week - 4h 30 min

### 9 March 2020
* *3h* Further cleaned up code and evaluated models
* *1h 30 min* Started working on "Implementation" and "Evaluation" chapters

### 13 March 2020
* *2h* Revisited how GradCAM works

### 14 March 2020
* *3h* Created a new network which outputs a heatmap and label

### 15 March 2020
* *3h* Evaluated new network

### 16 March 2020
* Finished coding, started working daily on dissertation *